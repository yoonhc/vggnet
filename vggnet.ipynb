{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet 구현 - 20192253 Hongchan Yoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device: ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Load CIFAR100 Dateset & perform PCA Analysis(밑의 RGB ColourShift에 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR100(root='./', train=True, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./', train=False, download=True)\n",
    "\n",
    "# Extract RGB pixel values from the training dataset\n",
    "pixels = np.vstack([np.asarray(img).reshape(-1, 3) for img, _ in train_dataset])\n",
    "\n",
    "'''VGGNet 논문- The only pre- processing we do is subtracting the mean RGB value, computed on the training set,\n",
    "from each pixel.'''\n",
    "# Compute the mean RGB value\n",
    "mean_rgb = np.mean(pixels, axis=0)\n",
    "\n",
    "# Compute the covariance matrix of the RGB pixel values\n",
    "cov_matrix = np.cov(pixels, rowvar=False)\n",
    "\n",
    "# Perform eigen decomposition to obtain eigenvectors and eigenvalues\n",
    "eig_vals, eig_vecs = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_indices = np.argsort(eig_vals)[::-1]\n",
    "eig_vals = eig_vals[sorted_indices]\n",
    "eig_vecs = eig_vecs[:, sorted_indices]\n",
    "\n",
    "print(\"Mean RGB values:\", mean_rgb)\n",
    "print(\"Eigenvalues:\", eig_vals)\n",
    "print(\"Eigenvectors:\\n\", eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Define Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2numpy(image):\n",
    "    if torch.is_tensor(image):\n",
    "        image = image.data.cpu().numpy()\n",
    "    else:\n",
    "        image = np.array(image)\n",
    "    return image\n",
    "\n",
    "'''\n",
    "VGGNet Paper - To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift\n",
    "\n",
    "다만 Dataset을 논문과 같이 ILSVRC-2014를 사용하지 않고, 이미지 크기가 32x32인 CIFAR100 을 사용할 것이기에 논문과 같이 Training Scale S에 맞춰 Resize후\n",
    "Crop 하는 방식은 사용하지 않고, 32x32 image 부분에 5x5 crop(구멍)을 내준다.\n",
    "'''\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, crop_pixel:int = 5):\n",
    "        self.crop_pixel = crop_pixel\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = convert2numpy(image)\n",
    "        # Image: Height x Width x Channel\n",
    "        x_y = np.random.choice(image.shape[0] - self.crop_pixel, 2)\n",
    "        start_x, start_y = x_y[0], x_y[1]\n",
    "        image[start_x: start_x + self.crop_pixel, start_y: start_y + self.crop_pixel, :] = 0.0\n",
    "\n",
    "        return image\n",
    "    \n",
    "\n",
    "# Random Horizontal Flipping\n",
    "# 'probability'의 확률로 좌우 flipping 실행\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, probability = 0.3):\n",
    "        assert probability >= 0.0 and probability <= 1.0\n",
    "        self.probability = probability\n",
    "\n",
    "    def __call__(self, image):\n",
    "        self.execute = np.random.rand() < self.probability\n",
    "        if self.execute:\n",
    "            new_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            return new_image\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "# Random RGB Colour Shift\n",
    "# VGGNet 논문에서는 RGB Colour Shift에 대한 자세한 내용은 없고, AlexNet 논문만 인용\n",
    "# 따라서 AlexNet의 RGB Colour Shif(PCA 연산 후 더하기)로 구현\n",
    "'''AlexNet - To each training image, we add multiples of the found principal components\n",
    "with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from\n",
    "a Gaussian with mean zero and standard deviation 0.1'''\n",
    "class RandomRGBColorShift(object):\n",
    "    def __init__(self, eig_vecs, eig_vals, alpha_std=0.1):\n",
    "        \"\"\"\n",
    "        Initialize with precomputed eigenvectors and eigenvalues.\n",
    "        \n",
    "        Parameters:\n",
    "        - eig_vecs: eigenvectors of the covariance matrix of RGB pixel values\n",
    "        - eig_vals: eigenvalues of the covariance matrix of RGB pixel values\n",
    "        - alpha_std: standard deviation of the Gaussian from which alphas are drawn\n",
    "        - probability: probability of applying the color shift\n",
    "        \"\"\"\n",
    "        self.eig_vecs = eig_vecs\n",
    "        self.eig_vals = eig_vals\n",
    "        self.alpha_std = alpha_std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        alpha = np.random.normal(0, self.alpha_std, 3)\n",
    "        quantity = np.dot(self.eig_vecs, alpha * self.eig_vals)\n",
    "        new_image = np.asarray(image).astype(np.float32)\n",
    "        for i in range(3):  # For R, G, B channels\n",
    "            new_image[:, :, i] += quantity[i]\n",
    "        new_image = np.clip(new_image, 0, 255).astype(np.uint8)\n",
    "        return new_image\n",
    "\n",
    "'''VGGNet - The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel\n",
    "'''\n",
    "class SubtractMeanRGB(object):\n",
    "    def __init__(self, mean):\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = np.asarray(image).astype(np.float32)\n",
    "        image -= self.mean\n",
    "        image = np.clip(image, 0, 255)\n",
    "        image = Image.fromarray(np.uint8(image))\n",
    "        return image\n",
    "    \n",
    "def imshow(image):\n",
    "    plt.imshow(np.transpose(image, (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Apply augmentation to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     RandomCrop(),\n\u001b[1;32m      3\u001b[0m     RandomHorizontalFlip(),\n\u001b[1;32m      4\u001b[0m     RandomRGBColourShift(eig_vecs, eig_vals),\n\u001b[1;32m      5\u001b[0m     SubtractMeanRGB(mean_rgb),\n\u001b[1;32m      6\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      7\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m32\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     11\u001b[0m test_transform \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     13\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m32\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                      (\u001b[38;5;241m0.247\u001b[39m, \u001b[38;5;241m0.243\u001b[39m, \u001b[38;5;241m0.261\u001b[39m))\n\u001b[1;32m     17\u001b[0m ])\n\u001b[1;32m     19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     20\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR100(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtrain_transform),\n\u001b[1;32m     21\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    #RandomRGBColorShift(eig_vecs, eig_vals),   잘 안되는 것 같아 밑의 ColorJitter로 대체\n",
    "    SubtractMeanRGB(mean_rgb),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    RandomCrop(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(32),\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    SubtractMeanRGB(mean_rgb),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(32),\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100(root='./', train=True, download=True, transform=train_transform),\n",
    "    batch_size=128, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100(root='./', train=False, download=True, transform=test_transform),\n",
    "    batch_size=128, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Plot the augmented images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "# Convert images to numpy for display\n",
    "images = images.numpy()\n",
    "\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard', \n",
    "    'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', \n",
    "    'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', \n",
    "    'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', \n",
    "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', \n",
    "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \n",
    "    'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', \n",
    "    'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
    "]\n",
    "\n",
    "# Plot the images in the batch\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "# Display 20 images\n",
    "# Viaulize Images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, int(20/2), idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VGGNet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. VGGNet Single Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGConvLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride=1):\n",
    "        super(VGGConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=stride, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. VGGNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, layer_infos, num_classes=100):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.conv_layers = self._make_layers(layer_infos)\n",
    "        self.fc_layers = nn.sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),   # image size에 따라 달라짐; 32(CIFAR10 image size)/32 = 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),                # 0.5 확률로 dropout\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def _make_layers(self, layer_infos):\n",
    "        layers = []\n",
    "        input_channels = 3                  # 처음은 RGB이므로 3\n",
    "        for layer_info in layer_infos:\n",
    "            if layer_info == 'M':\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                layers.append(VGGConvLayer(input_channels, layer_info))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc_layers(out)\n",
    "        \n",
    "VGG19 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "\n",
    "def VGGNet19():\n",
    "    return VGGNet(VGG19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Save & Load model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_name):\n",
    "    path = './' + str(save_name) + '.pt'\n",
    "    ckpt = {'model': model}\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "def load_model(init_model, load_name):\n",
    "    path = './' + str(load_name) + '.pt'\n",
    "    load_file = torch.load(path, map_location='cpu')\n",
    "    model = load_file['model']\n",
    "    init_model.load_state_dict(model.state_dict())\n",
    "    return init_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Train & Test Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(total_epoch, network, criterion, optimizer, lr_schedule, train_loader, device = 'cpu', save_name = 'save_name'):\n",
    "    network = network.to(device)\n",
    "    for epoch in range(total_epoch):\n",
    "        train_single_epoch(epoch, network, criterion, optimizer, train_loader, device)\n",
    "        lr_schedule.step()\n",
    "        if ((epoch + 1) % 10 == 0) or epoch == total_epoch - 1:\n",
    "            save_model(network, save_name)\n",
    "            print('Model saved at epoch {} with name {} '.format(epoch + 1, save_name + '.pt'))\n",
    "\n",
    "def train_single_epoch(current_epoch, network, criterion, optimizer, train_loader, device='cpu'):\n",
    "    network.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total_sample = 0.0, 0.0\n",
    "    for idx, (input, label) in enumerate(train_loader):\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = network(input)\n",
    "\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        correct += (pred == label).sum().item()\n",
    "        total_sample += label.size(0)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print('Epoch: {} | Training Accuracy: {:.2f} % | Loss: {:.2f}'.format(current_epoch, 100*correct/total_sample, running_loss/(idx+1)))\n",
    "\n",
    "\n",
    "def test(network, criterion, test_loader, device = 'cpu', load_name = None):\n",
    "    if load_name is not None:\n",
    "        network = load_model(network, load_name)\n",
    "\n",
    "    print('Test start')\n",
    "    network = network.to(device)\n",
    "    network.eval()\n",
    "    test_loss = 0.0\n",
    "    correct, total_sample = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, label) in enumerate(test_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            output = network(image)\n",
    "            loss = criterion(output, label)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            print('Prediction values: {}' .format(pred))\n",
    "            total_sample += label.size(0)\n",
    "            correct += (pred == label).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    print('Test loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss/(idx + 1) ,correct, total_sample, 100 * correct / total_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Train & Test VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = VGGNet19()\n",
    "total_epoch = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma =0.1)\n",
    "\n",
    "saved_model_name = '20192253_VGGNet19'\n",
    "\n",
    "train(total_epoch, network, criterion, optimizer, step_lr_scheduler, train_loader, device = device, save_name = saved_model_name)\n",
    "test(network, criterion, test_loader, device = device, load_name = saved_model_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
